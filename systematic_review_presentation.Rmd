---
title: Systematic Review with Active Learning
knit: (function(input_file, encoding) {
  out_dir <- 'docs';
  rmarkdown::render(input_file,
 encoding=encoding,
 output_file=file.path(dirname(input_file), out_dir, 'index.html'))})
Author: André Pirralha
output: 
 knitrBootstrap::bootstrap_document:
    title: "Systematic Review with Active Learning"
    theme: flatly
    highlight: sunburst
    theme.chooser: FALSE
    highlight.chooser: FALSE
 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

The main goal of this document is to introduce systematic reviews, explain why they are useful and present a workflow to conduct systematic reviews using artificial intelligence to assist in the task of finding relevant texts.

# What is a systematic review?

"Systematic literature reviews are a method of making sense of large bodies of information, and a means of contributing to the answers to questions about what works and what does not – and many other types of question too. They are a method of mapping out areas of uncertainty, and identifying where little or no relevant research has been done, but where new studies are needed. Systematic reviews also flag up areas where spurious certainty abounds. These are areas where we think we know more than we do, but where in reality there is little convincing evidence to support our beliefs."[(Petticew & Roberts, 2005, pp. 2](https://doi.org/10.1002/9780470754887)) 


## Systematic review vs meta-analysis vs narrative review

"A systematic review answers strives to comprehensively identify, appraise, and synthesize all the relevant studies on a given topic. Systematic reviews are often used to test just a single hypothesis, or a series of related hypotheses."

"A meta-analysis uses a specific statistical technique for synthesizing the results of several studies into a single quantitative estimate (i.e., a summary effect size)."

"A narrative review is the process of synthesizing primary studies and exploring heterogeneity descriptively, rather than statistically." 

[(Petticew & Roberts, 2005, pp. 19](https://doi.org/10.1002/9780470754887))



## Why do a systematic review?

It is a fair criticism of scientific research that it is often difficult to know which, if any, study to believe.

> Newton’s Third Law of Motion: ‘‘For every expert there is an equal and opposite expert.’’

The traditional scientific approach to this problem is to carry out a literature review. However, traditional literature reviews frequently summarize unrepresentative samples of studies in an unsystematic fashion. But the problem is not just one of inconsistency, but one of information overload. The massive expansion of research output, both in peer-reviewed publications, and unpublished, e.g. in conference presentations or symposia, mean it is difficult to establish what work has been done in your area already, and to ensure that knowledge keeps up to date with the best research evidence. 

## The role of systematic literature reviews

Systematic reviews are literature reviews that adhere closely to a set of scientific methods that explicitly aim to limit systematic error (bias), mainly by attempting to identify, appraise and synthesize all relevant studies (of whatever design) in order to answer a particular question (or set of questions).


Systematic review will be most useful when:

• there is a clear and defined research question;

• several empirical studies have been published;

• there is uncertainty about the results.


***


# Tutorial: My first systematic review

A systematic review needs a detailed protocol that describes how the review will be processed and methods that will be applied. There are several published examples of such protocols. Here we follow the protocol suggested by [Bramer et al. (2018)](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6148622/) which consists on the following steps:

1. Determine a clear and focused question

2. Describe the articles that can answer the question

3. Decide which key concepts address the different elements of the question

4. Decide which elements should be used for the best results

5. Choose an appropriate database and interface to start with

6. Document the search process in a text document

7. Identify appropriate index terms in the thesaurus of the first database

8. Identify synonyms in the thesaurus

9. Add variations in search terms

10. Use database-appropriate syntax, with parentheses, Boolean operators, and field codes

11. Optimize the search

12. Evaluate the initial results

13. Check for errors

14. Translate to other databases

15. Test and reiterate


Note: This tutorial will not follow through with an exhaustive account of each of these steps. Instead, a shortened version with some steps of this list will presented and discussed. 

## 1. Determine a clear and focused question 

For the sake of this exercise, the aim is to answer the following question: 

>What factors explain interview duration in computer assisted telephone surveys?

## 2. Describe the articles that can answer the question


Research articles that collect and analize interview duration with computer assisted telephone surveys (CATI) data.

## 3. What are the research question key concepts?

- Survey

- Telephone

- Interview duration

## 5. Choose the database

There is a very long list of [academic databases and search engines](https://en.wikipedia.org/wiki/List_of_academic_databases_and_search_engines).

> How to choose the database? 

In addition to any specific requirements which might come from the specificity of the research question (e.g. medical trials have dedicated databases), recent research by [Gusenbauer and Haddaway (2019](https://onlinelibrary.wiley.com/doi/full/10.1002/jrsm.1378)) suggests the following databases:  

* [ACM Digital Library](https://dl.acm.org/) 
* [BASE](https://www.base-search.net/) 
* [ClinicalTrials.gov](https://www.clinicaltrials.gov/) 
* [Cochrane Library](https://www.cochranelibrary.com/) 
* [EbscoHost](https://www.ebsco.com/products/research-databases) (tested for ERIC, Medline, EconLit, CINHAL Plus, SportsDiscus)
* [OVID](https://www.ovid.com/) (tested for Embase, Embase Classic, PsychINFO) 
* [ProQuest](https://www.proquest.com/index) (tested for Nursing & Allied Health Database, Public Health Database)
* [PubMed](https://pubmed.ncbi.nlm.nih.gov/) 
* [ScienceDirect](https://www.sciencedirect.com/) 
* [Scopus](https://scopus.com/) 
* [TRID](https://trid.trb.org/) 
* [Virtual Health Library](https://www.who.int/chp/knowledge/vhl/en/) 
* [Web of Science](https://webofknowledge.com/WOS) (tested for Web of Science Core Collection, Medline) 
* [Wiley Online Library](https://onlinelibrary.wiley.com/)


**[Gusenbauer and Haddaway (2019](https://onlinelibrary.wiley.com/doi/full/10.1002/jrsm.1378)) further suggest that systems such as Google scholar have severe performance limitations and should only be considered supplementary to these latter databases, especially for non-query‐based search methods where they might still provide great benefit.**

***

In this exercise we use the [Web of Science Core Collection](https://www.webofknowledge.com)

## 6. Document the search process in a text document

Keep a document with a log of the all the search/query steps in order to make it reproducible.

## 7. Identify appropriate index terms in the thesaurus of the first database

The initial terms we identify are the following:

* Duration
* Telephone
* Survey


## 8. Identify synonyms & 9. Add variations in search terms

* Timestamps
* Interview lenght
* CATI

## 10. Use database-appropriate syntax, with parentheses, Boolean operators, and field codes

Check the [advance search](http://images.webofknowledge.com/WOKRS535R100/help/WOS/hp_advanced_search.html) help menu of Web of Science.

The start of our query will be the following:

>ALL=(survey  AND  telephone  AND  duration)  

![Select language and document type](./img/optimize.png){width="120%"}


## 11. Optimize the search

We can start delimiting and optimizing our search by, for example, selecting the language and the type of documents.

![Results](./img/optimize2.png){width=120%}


Next we can add the synonyms we previously identified.

>ALL=(survey OR surveys)  AND  ALL=(telephone  OR  CATI)  AND  ALL=(duration  OR  length  OR  timestamps)

![Add synonyms](./img/optimize3.png){width=120%}



## 14. Translate to other databases

Extend the search/query to other databases.

***

# Selecting the relevant literature

So now we got the data and we have 1227 entries. Do we have to read all this papers?

**Short answer: yes.** 

However, it is very likely that a high proportion of these entries are actually not relevant to our research question. We can look for the help of active learning (artificial intelligence) systems  to reduce the number of papers we actually have to read or at least to identify the most relevant papers.

## Presenting the ASReview

The [ASReview](https://asreview.nl/) enables you to screen more texts than the traditional way of screening literature reviews in the same amount of time. How?


![Illustration of ASReview](./img/FlowChart.png){width=70%}


ASReview helps by ["‘simply’ (...) continuously reorder the stack of records based on relevance scores computed by a machine learning model. By reordering the records based on predicted relevance, you will find the relevant records way sooner than the traditional way."](https://asreview.nl/asreview-class-101/)  



![graph)](./img/asreview.mp4)

Source: [ASReview-class-101](https://asreview.nl/asreview-class-101/)

Detailed information regarding how to install and other related issues, such as file formats, can be found in the [ASReview](https://asreview.nl/) website and [documentation](https://readthedocs.org/projects/asreview/downloads/pdf/stable/). 



## Example: using the ASreview

After compiling the results of the systematic review we conducted before to a [compatible data format file](https://asreview.readthedocs.io/en/latest/intro/datasets.html) we are ready to introduce it into ASReview.


![ASReview](./img/asreview1.png){width=80%}



Then we must set the priors to train the active learning system. This is done by looking at the abstracts and selecting the ones that are relevant or irrelevant to our research question. 

An efficient way to do this is by searching for a specific document you already know is relevant.

![Relevant](./img/asreview2.png){width=80%}






![Relevant](./img/asreview3.png){width=80%}

After classifying some documents (the more, the better)  we can select the model of our preference. More information about the specificities of each of these training models can be found in the [ASReview](https://asreview.readthedocs.io/en/latest/features/pre_screening.html#select-model) website.

![Learning Model](./img/asreview4.png){width=80%}



![Statistics](./img/asreview5.png){width=80%}




## Final Results

In the final results we can see all the 1227 entries of the literature review now ranked by relevance following the priors we set in the active learning model. 


![Excel](./img/excel1.png)


```{r, echo=FALSE, warning=FALSE}


library(readxl)
library(knitr)
library(kableExtra)


df <- read_excel("asreview_result_interview-length.xlsx")


```


Each entry holds the following paper metadata:

```{r, echo=FALSE, warning=FALSE}


#kable(df[1:2, ], format = "html") 

x<-kable(df[1:2, ], format = "html") 
column_spec(x,10,width_min = "40em")

#kable(df[1:2, ])



```



# Further information

[PRISMA](http://www.prisma-statement.org/): how to report systematic reviews (and metanalyses)


